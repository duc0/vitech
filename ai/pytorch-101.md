Deep learning (3): PyTorch vÃ  MNIST
-----------------------------------

TÃ¡c giáº£: Nguyá»…n XuÃ¢n KhÃ¡nh

**Nguá»“n**: [KhÃ¡nh's blog] (http://khanhxnguyen.com/deep-learning-3/)


ChÃ o cÃ¡c báº¡n, hÃ´m nay chÃºng ta sáº½ nghiÃªn cá»©u cáº¥u trÃºc má»™t chÆ°Æ¡ng trÃ¬nh deep learning cÆ¡ báº£n. Náº¿u khi thá»±c hÃ nh cÃ³ khÃ³ khÄƒn, cÃ¡c báº¡n comment vÃ o dÆ°á»›i bÃ i viáº¿t hoáº·c vÃ oÂ [group machine learners](https://www.facebook.com/groups/485581088316769/)Â Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£.

0\. PyTorch:

Äáº§u tiÃªn, chÃºng ta cáº§n chá»n má»™t ná»n táº£ng deep learning Ä‘á»ƒ thá»±c hÃ nh. MÃ¬nh chá»nÂ [PyTorch](https://github.com/pytorch/pytorch)Â vÃ¬ nÃ³ cÃ³ nhá»¯ng Æ°u Ä‘iá»ƒm sau:

-- ThÃ¢n thiá»‡n: PyTorch cho phÃ©p theo dÃµi Ä‘Æ°á»£c cÃ¡c tham sá»‘ cá»§a model trong khi cháº¡y, giÃºp debug Ä‘Æ°á»£c thuáº­n tiá»‡n hÆ¡n.

-- Linh Ä‘á»™ng: PyTorch cho phÃ©p thay Ä‘á»•i cáº¥u trÃºc model khi cháº¡y.

-- Large scale: PyTorch há»— trá»£ cháº¡y vá»›i nhiá»u GPU má»™t cÃ¡ch Ä‘Æ¡n giáº£n.

-- Äá»™i ngÅ© phÃ¡t triá»ƒn: PyTorch Ä‘Æ°á»£c Facebook, Twitter vÃ  cÃ¡c trÆ°á»ng Ä‘áº¡i há»c lá»›n phÃ¡t triá»ƒn vÃ¬ tháº¿ sáº½ luÃ´n cáº­p nháº­t nhá»¯ng cÃ´ng nghá»‡ Ä‘á»‰nh nháº¥t tá»« giá»›i nghiÃªn cá»©u.

1\. CÃ i Ä‘áº·t PyTorch:

MÃ¬nh máº·c Ä‘á»‹nh lÃ Â UbuntuÂ vÃ Â PythonÂ Ä‘Ã£ Ä‘Æ°á»£c cÃ i sáºµn.

a. CÃ i Ä‘áº·t CUDA vÃ  CuDNN:

Lá»±a chá»n nÃ yÂ chá»‰ dÃ nh cho ngÆ°á»i cÃ³ GPU.

[á» Ä‘Ã¢y](https://yangcha.github.io/GTX-1080/)Â cÃ³ má»™t hÆ°á»›ng dáº«n chi tiáº¿t Ä‘á»ƒ cÃ i Ä‘áº·t mÃ¡y vá»›i GPU GTX 1080. CÃ¡c báº¡n nhá»›Â thay Ä‘á»•i tÃªn fileÂ tÃ¹y theo version cá»§a CUDA vÃ  CuDNN Ä‘Æ°á»£c táº£i vá». Náº¿u cÃ¡c báº¡n Ä‘Ã£ hoÃ n thÃ nh háº¿t cÃ¡c bÆ°á»›c trong Ä‘Ã³ thÃ¬ chuyá»ƒn qua má»¥c (b) luÃ´n mÃ  khÃ´ng cáº§n Ä‘á»c tiáº¿p.

DÆ°á»›i Ä‘Ã¢y mÃ¬nh chá»‰ cÃ¡ch táº£i cÃ¡c file CUDA vÃ  CuDNN vá» sao cho Ä‘Ãºng.

Äá»ƒ cÃ i Ä‘áº·t CUDA, cÃ¡c báº¡n google CUDA rá»“i vÃ o trang chá»§, chá»n cáº¥u hÃ¬nh mÃ¡y cho phÃ¹ há»£p rá»“i táº£i file vá». MÃ¬nh thÃ¬ thÆ°á»ng hay táº£i file .deb vá» vÃ¬ nÃ³ dá»… cÃ i Ä‘áº·t. LÆ°u Ã½ lÃ  file khÃ¡ náº·ng (gáº§n 2GB).

![](https://i2.wp.com/khanhxnguyen.com/wp-content/uploads/2017/03/Selection_013.png?resize=640%2C747)

Sau khi táº£i xong, cÃ¡c báº¡n vÃ o thÆ° má»¥c cÃ³ file vá»«a táº£i vÃ  cháº¡y 3 dÃ²ng lá»‡nh nhÆ° hÆ°á»›ng dáº«n trong terminal.

|

1

2

3

 |

`sudo` `dpkg -i FILE_CUDA.deb`

`sudo` `apt-get update`

`sudo` `apt-get` `install` `cuda`

 |

LÆ°u Ã½ lÃ  dÃ²ng lá»‡nh Ä‘áº§uÂ thay Ä‘á»•i tÃ¹y theo tÃªn fileÂ nÃªn tá»‘t nháº¥t cá»© copy paste vÃ o terminal nhÃ©.

CuDNN Ä‘Ã²i há»i cÃ¡c báº¡n pháº£i Ä‘Äƒng kÃ½, Ä‘iá»n thÃ´ng tin vÃ  Ä‘á»£i email cháº¥p nháº­n má»›i táº£i Ä‘Æ°á»£c. Trang táº£i CuDNN sáº½ trÃ´ng nhÆ° sau:

![](https://i1.wp.com/khanhxnguyen.com/wp-content/uploads/2017/03/Selection_015.png?resize=552%2C280)

MÃ¬nh táº£i vá» gÃ³i Ä‘Æ°á»£c gáº¡ch dÆ°á»›i trong hÃ¬nh trÃªn. Sau khi táº£i vá», cÃ¡c báº¡n cháº¡y nhá»¯ng cÃ¢u lá»‡nh sau:

|

1

2

3

4

 |

`tar` `xvzf FILE_CUDNN.tgz`

`sudo` `cp` `cuda``/include/cudnn``.h` `/usr/local/cuda/include`

`sudo` `cp` `cuda``/lib64/libcudnn``*` `/usr/local/cuda/lib64`

`sudo` `chmod` `a+r` `/usr/local/cuda/include/cudnn``.h` `/usr/local/cuda/lib64/libcudnn``*`

 |

b. CÃ i Ä‘áº·t PyTorch:

CÃ¡c báº¡n vÃ oÂ [trang chá»§ cá»§a PyTorch](http://pytorch.org/)Â vÃ  lá»±a chá»n cÃ¡ch cÃ i Ä‘áº·t cho phÃ¹ há»£p. VÃ­ dá»¥ mÃ¬nh hay lá»±a chá»n cÃ¡ch cÃ i nhÆ° sau:

![](https://i0.wp.com/khanhxnguyen.com/wp-content/uploads/2017/03/Selection_017.png?resize=640%2C347)

Sau Ä‘Ã³ cÃ¡c báº¡n copy 2 dÃ²ng lá»‡nh á»Ÿ Ã´ "run this command" vÃ  cháº¡y chÃºng trong terminal. Náº¿u khi cÃ i cÃ¡c báº¡n bá»‹ bÃ¡o lá»—i chÆ°a cÃ i package manager nÃ o thÃ¬ google cÃ¡ch cÃ i package manager Ä‘Ã³ nhÃ© (vÃ­ dá»¥ google "install pip ubuntu").

CÃ²n má»™t cÃ¡ch cÃ i ná»¯a Ä‘Ã³ lÃ  cÃ¡c báº¡n clone repo cá»§a PyTorch vá» mÃ¡yÂ (clone lÃ  má»™t thuáº­t ngá»¯ cá»§aÂ [git](http://rogerdudler.github.io/git-guide/), chá»‰ viá»‡c download code vá» mÃ¡y) báº±ng cÃ¡ch má»Ÿ terminal ra vÃ  gÃµ dÃ²ng lá»‡nh sau:

|

1

 |

`git clone https:``//github``.com``/pytorch/pytorch``.git`

 |

Sau Ä‘Ã³ lÃ m theo hÆ°á»›ng dáº«nÂ [á»Ÿ Ä‘Ã¢y](https://github.com/pytorch/pytorch#installation). HÆ°á»›ng dáº«n nÃ y bao gá»“m cáº£ viá»‡cÂ cÃ i Ä‘áº·t khÃ´ng cáº§n CUDA. CÃ¡ch báº¡n chá»‰ cáº§n Ä‘á»‹nh nghÄ©a má»™t biáº¿n NO_CUDA trong terminal trÆ°á»›c khi cÃ i Ä‘áº·t.

|

1

 |

`export` `NO_CUDA=1`

 |

2\. Táº£i code vÃ­ dá»¥:

Äá»ƒ há»c PyTorch hiá»‡u quáº£, cÃ¡c báº¡n nÃªn bá» thá»i gian ra xem cÃ¡cÂ [code vÃ­ dá»¥ máº«u](https://github.com/pytorch/examples)Â Ä‘Æ°á»£c cÃ¡c chuyÃªn gia PyTorch viáº¿t sáºµn.

á» Ä‘Ã¢y, mÃ¬nh copy vÃ­ dá»¥ MNIST vá» vÃ  sá»­a láº¡i Ä‘á»ƒ Ä‘Æ¡n giáº£n hÃ³a má»™t sá»‘ váº¥n Ä‘á» táº£i dá»¯ liá»‡u. CÃ¡c báº¡n clone repoÂ [mnist-pytorch](https://github.com/khanhptnk/mnist-pytorch)Â cá»§a mÃ¬nhÂ vá» mÃ¡y:

|

1

 |

`git clone https:``//github``.com``/khanhptnk/mnist-pytorch``.git`

 |

Sau Ä‘Ã³ trong thÆ° má»¥c hiá»‡n hÃ nh cá»§a báº¡n sáº½ xuáº¥t hiá»‡n thÆ° má»¥c "mnist-pytorch". Kiá»ƒm tra Ä‘iá»u Ä‘Ã³ báº±ng cÃ¡ch dÃ¹ng lá»‡nh:

|

1

 |

`ls` `. |` `grep` `mnist-pytorch`

 |

Sau Ä‘Ã³ cÃ¡c báº¡n Ä‘i vÃ o thÆ° má»¥c nÃ y:

|

1

2

 |

`cd` `mnist-pytorch`

`ls` `-1`

 |

3\. MNIST:

MNIST lÃ  má»™t bÃ i toÃ¡n nháº­n diá»‡n chá»¯ sá»‘ viáº¿t tay thÃ´ng qua hÃ¬nh áº£nh. Input vÃ o lÃ  má»™t áº£nh tráº¯ng Ä‘en cá»§a má»™t chá»¯ sá»‘ viáº¿t tay tá»« 0 Ä‘áº¿n 9. Nhiá»‡m vá»¥ cá»§a model lÃ  dá»± Ä‘oÃ¡n xem táº¥mÂ áº£nh Ä‘Ã³ biá»ƒu thá»‹ sá»‘ nÃ o. VÃ­ dá»¥ nhÆ° Ä‘Ã¢y lÃ  sá»‘ 2:

![](https://i2.wp.com/khanhxnguyen.com/wp-content/uploads/2017/03/mnist-2.png?resize=165%2C166)

MNIST Ä‘Æ°á»£c sá»­ dá»¥ng nhÆ° má»™t bÃ i táº­p dáº¡ng "Hello world" cho deep learning. MNIST lÃ  má»™t bÃ i toÃ¡n kinh Ä‘iá»ƒn vá»Â multiclass classification, tá»©c lÃ  phÃ¢n loáº¡i cÃ³ nhiá»u loáº¡i nhÃ£n. Multiclass classification lÃ  má»™t dáº¡ng supervised learning, tá»©c lÃ  má»—i inputÂ xxÄ‘Æ°á»£c gáº¯n vá»›i má»™t loáº¡i label (nhÃ£n)Â yy. LabelÂ yyÂ Ä‘Æ°á»£c cho biáº¿t trong lÃºc huáº¥n luyá»‡n. LabelÂ yyÂ nháº­n cÃ¡c giÃ¡ trá»‹ tá»« táº­p rá»i ráº¡cÂ YYÂ cÃ³Â nhiá»u hÆ¡n 2 pháº§n tá»­Â  (náº¿uÂ YYÂ chá»‰ cÃ³ hai pháº§n tá»­ ngÆ°á»i ta gá»i lÃ Â binary classification). Trong trÆ°á»ng há»£p á»Ÿ Ä‘Ã¢y thÃ¬Â xxÂ chÃ­nh lÃ  táº¥m áº£nh,Â yyÂ lÃ  con sá»‘ táº¥m áº£nh biá»ƒu thá»‹, cÃ²nÂ YYÂ lÃ  táº­p há»£p cÃ¡c sá»‘ tá»« 0 Ä‘áº¿n 9.

Trong thÆ° má»¥c "mnist-pytorch", báº¡n cháº¡y thá»­ chÆ°Æ¡ng trÃ¬nh nhÆ° sau:

|

1

 |

`python main.py`

 |

Náº¿u cháº¡y thÃ nh cÃ´ng báº¡n sáº½ nhÃ¬n tháº¥y output nhÆ° sau:

![](https://i2.wp.com/khanhxnguyen.com/wp-content/uploads/2017/03/Selection_016.png?resize=640%2C798)

Giáº£i thÃ­ch output nÃ y má»™t chÃºt. Model sáº½ Ä‘i qua háº¿t cÃ¡c vÃ­ dá»¥ cá»§a táº­p train nhiá»u láº§n. Má»—i láº§n nhÆ° váº­y Ä‘Æ°á»£c gá»i lÃ  má»™tÂ epoch. Trong má»—i epoch, Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»› táº­p train sáº½ Ä‘Æ°á»£c chia nhá» thÃ nh cÃ¡cÂ batch. á»Ÿ Ä‘Ã¢y cÃ¡c báº¡n tháº¥y lÃ  táº­p train gá»“m 60000 vÃ­ dá»¥.Â Batch size, tá»©c lÃ  sá»‘ lÆ°á»£ng vÃ­ dá»¥ trong má»—i batch, lÃ  64. Váº­y thÃ¬ Ä‘á»ƒ Ä‘i háº¿t cÃ¡c vÃ­ dá»¥ ta cáº§n 60000 / 64 batch. Batch thá»© nháº¥t gá»“m cÃ¡c vÃ­ dá»¥ tá»« 1 Ä‘áº¿n 64, batch thá»© hai tá»« 65 Ä‘áº¿n 128, vÃ  cá»© tháº¿. Batch cuá»‘i cÃ¹ng thÃ¬ cÃ³ thá»ƒ nhá» hÆ¡n cÃ¡c batch cÃ²n láº¡i náº¿u nhÆ° sá»‘ lÆ°á»£ng vÃ­ dá»¥ khÃ´ng chia háº¿t cho batch size. Sá»Ÿ dÄ© pháº£i chia thÃ nh batch nhÆ° váº­y lÃ  vÃ¬ bá»™ nhá»› cá»§a mÃ¡y tÃ­nh cÃ³ háº¡n. CÃ¡c báº¡n dÃ¹ng batch size=64 lÃ  Ä‘á»§ tá»‘t cho Ä‘a sá»‘ cÃ¡c á»©ng dá»¥ng. CÅ©ng lÆ°u Ã½ lÃ  batch size thÆ°á»ng sáº½ lÃ  má»™t lÅ©y thá»«a nÃ o Ä‘Ã³ cá»§a 2 Ä‘á»ƒ sá»­ dá»¥ng bá»™ nhá»› Ä‘Æ°á»£c hiá»‡u quáº£.

ChÆ°Æ¡ng trÃ¬nhÂ in raÂ loss, tá»©c lÃ  giÃ¡ trá»‹ cá»§aÂ [hÃ m máº¥t mÃ¡t](http://khanhxnguyen.com/loss-function/), sau má»—i 200 batch (log interval báº±ng 200). VÃ¬ batch size lÃ  64, nÃªn nÃ³i cÃ¡ch khÃ¡c, loss Ä‘Æ°á»£c sau má»—i 200 * 64 = 12800 vÃ­ dá»¥. HÃ m máº¥t mÃ¡t Ä‘Æ°á»£c dÃ¹ng á»Ÿ Ä‘Ã¢y lÃ  gÃ¬ mÃ¬nh sáº½ nÃ³i trong bÃ i sau. Hiá»‡n giá» cÃ¡c báº¡n chá»‰ cáº§n hiá»ƒu nÃ³ thá»ƒ hiá»‡n cho Ä‘á»™ chÃ­nh xÃ¡c cá»§a model khi huáº¥n luyá»‡n. LÆ°u Ã½ lÃ  loss á»Ÿ Ä‘Ã¢y chá»‰ lÃ  giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t cho batch hiá»‡n thá»i thÃ´i, khÃ´ng pháº£i cá»§a cáº£ táº­p train. Sau cÃ¹ng chÃºng ta má»›i in raÂ average lossÂ trÃªn train set, Ä‘Æ°á»£c tÃ­nh báº±ng trung bÃ¬nh cá»™ng cá»§a cÃ¡c loss cá»§a táº¥t cáº£ cÃ¡c batch.

Sau má»—i epoch, chÃºng ta in ra average loss trÃªn táº­p test. ÄÃ¢y lÃ  trung bÃ¬nh giÃ¡ trá»‹ hÃ m máº¥t mÃ¡t trÃªn táº¥t cáº£ cÃ¡c vÃ­ dá»¥ cá»§a táº­p test, chá»© khÃ´ng pháº£i táº­p train. Náº¿u cÃ¡c báº¡n tháº¯c máº¯c táº¡i sao láº¡i lÃ  táº­p test, hÃ£y xem láº¡i quy trÃ¬nh supervised learning á»ŸÂ [bÃ i nÃ y](http://khanhxnguyen.com/overfit-2/). LÆ°u Ã½ lÃ  vÃ¬ á»Ÿ Ä‘Ã¢y chÃºng ta khÃ´ng cÃ³ hyperparameter nÃªn khÃ´ng cáº§n táº­p dev. ChÃºng ta Ä‘Ã¡nh giÃ¡ model trÃªn táº­p test luÃ´n. Quan sÃ¡t average loss trÃªn táº­p train vÃ  táº­p test giÃºp chÃºng ta xÃ¡c Ä‘á»‹nh xem model cÃ³ bá»‹Â [overfit](http://khanhxnguyen.com/machine-learning-101-overfit/)Â hay khÃ´ng. á» Ä‘Ã¢y ta thÃ¢y cáº£ hai Ä‘á»u Ä‘ang giáº£m sau má»—i epoch, tá»©c lÃ  model Ä‘ang khÃ´ng bá»‹ overfit.

Tuy nhiÃªn, vá»›i bÃ i toÃ¡n nÃ y, giÃ¡ trá»‹ chÃºng ta quan tÃ¢m hÆ¡n khi so sÃ¡nh Ä‘á»™ tá»‘t giá»¯a cÃ¡c model lÃ Â accuracy. Accuracy lÃ  pháº§n trÄƒm sá»‘ lÆ°á»£ng vÃ­ dá»¥ trong táº­p test mÃ  model Ä‘oÃ¡n Ä‘Ãºng label. VÃ¬ MNIST lÃ  má»™t bÃ i toÃ¡n khÃ¡ dá»… nÃªn model Ä‘áº¡t accuracy gáº§n nhÆ° tuyá»‡t Ä‘á»‘i (98%) chá»‰ sau 5 epoch.

4\. Thay Ä‘á»•i flag:

CÃ¢u há»i Ä‘áº·t ra lÃ  lÃ m Ä‘á»ƒ thay cÃ¡c thÃ´ng sá»‘ nhÆ° lÃ  sá»‘ lÆ°á»£ng epoch hay batch size? Náº¿u cÃ¡c báº¡n nhÃ¬n vÃ o fileÂ [main.py](https://github.com/khanhptnk/mnist-pytorch/blob/master/main.py)Â trong thÆ° má»¥c "mnist-pytorch" vá»«a táº£i vá» thÃ¬ cÃ¡c dÃ²ng Ä‘áº§u tiÃªn sáº½ trÃ´ng giá»‘ng nhÆ° tháº¿ nÃ y:

|

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

 |

`parser` `=` `argparse.ArgumentParser(description``=``'PyTorch MNIST Example'``)`

`parser.add_argument(``'--no_download_data'``, action``=``'store_true'``, default``=``False``,`

`help``=``'Do not download data'``)`

`parser.add_argument(``'--batch-size'``,` `type``=``int``, default``=``64``, metavar``=``'N'``,`

`help``=``'input batch size for training (default: 64)'``)`

`parser.add_argument(``'--test-batch-size'``,` `type``=``int``, default``=``1000``, metavar``=``'N'``,`

`help``=``'input batch size for testing (default: 1000)'``)`

`parser.add_argument(``'--epochs'``,` `type``=``int``, default``=``5``, metavar``=``'N'``,`

`help``=``'number of epochs to train (default: 10)'``)`

`parser.add_argument(``'--lr'``,` `type``=``float``, default``=``0.01``, metavar``=``'LR'``,`

`help``=``'learning rate (default: 0.01)'``)`

`parser.add_argument(``'--momentum'``,` `type``=``float``, default``=``0.5``, metavar``=``'M'``,`

`help``=``'SGD momentum (default: 0.5)'``)`

`parser.add_argument(``'--no-cuda'``, action``=``'store_true'``, default``=``False``,`

`help``=``'enables CUDA training'``)`

`parser.add_argument(``'--seed'``,` `type``=``int``, default``=``1``, metavar``=``'S'``,`

`help``=``'random seed (default: 1)'``)`

`parser.add_argument(``'--log-interval'``,` `type``=``int``, default``=``200``, metavar``=``'N'``,`

`help``=``'how many batches to wait before logging training status'``)`

`args` `=` `parser.parse_args()`

`args.cuda` `=` `not` `args.no_cuda` `and` `torch.cuda.is_available()`

 |

ÄÃ¢y lÃ  nÆ¡i Ä‘á»‹nh nghÄ©a cÃ¡cÂ flag. Flag lÃ  cÃ¡ch cÃ¡c báº¡n truyá»n thÃ´ng sá»‘ vÃ o chÆ°Æ¡ng trÃ¬nh Ä‘á»ƒ cháº¡y model vá»›i nhiá»u cáº¥u hÃ¬nh khÃ¡c nhau. Flag Ä‘Æ°á»£c máº·c Ä‘á»‹nh sáºµn cÃ¡c giÃ¡ trá»‹ báº±ng tham sá»‘ "default" khi nÃ³ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a. VÃ­ dá»¥ nhÆ° á»Ÿ trÃªn mÃ¬nh thá»±c sá»± Ä‘ang cháº¡y chÆ°Æ¡ng trÃ¬nh vá»›i cÃ¢u lá»‡nh:

|

1

 |

`python main.py --batch_size=64 --epochs=5 --log-interval=200`

 |

(cÃ³ nhiá»u flag khÃ¡c nhÆ° khÃ´ng ghi háº¿t Ä‘á»ƒ tiáº¿t kiá»‡m khÃ´ng gian.)

Tuy nhiÃªn thÃ¬ cÃ¡c giÃ¡ trá»‹ cá»§a cÃ¡c flag nÃ y láº¡i trÃ¹ng vá»›i giÃ¡ trá»‹ máº·c Ä‘á»‹nh nÃªn khÃ´ng ghi cÅ©ng khÃ´ng sao. CÃ¡c báº¡n cÃ³ thá»ƒ thay Ä‘á»•i giÃ¡ trá»‹ cÃ¡c flag báº±ng viá»‡c thay Ä‘á»•i giÃ¡ trá»‹ sau dáº¥u "=". VÃ­ dá»¥ mÃ¬nh muá»‘n cháº¡y model vá»›i 10 epoch vÃ  batch size 128 thÃ¬ lÃ m nhÆ° sau.

|

1

 |

`python main.py --batch_size=128 --epochs=10`

 |

BÃ i táº­p:Â *hÃ£y tÃ¬m hiá»ƒu Ã½ nghÄ©a cá»§a cÃ¡c flag khÃ¡c vÃ  thá»­ thay Ä‘á»•i chÃºng xem Ä‘iá»u gÃ¬ xáº£y ra.*

5\. PhÃ¢n tÃ­ch code:

BÃ¢y giá» chÃºng ta sáº½ cÃ¹ng Ä‘i sÃ¢u vÃ oÂ [main.py](https://github.com/khanhptnk/mnist-pytorch/blob/master/main.py)Â Ä‘á»ƒ há»c bá»‘ cá»¥c cá»§a má»™t chÆ°Æ¡ng trÃ¬nh deep learning. Äiá»u Ä‘áº§u tiÃªn cÃ¡c báº¡n cÃ³ thá»ƒ tháº¥y lÃ  chÆ°Æ¡ng trÃ¬nh vÃ´ cÃ¹ng ngáº¯n nhá» vÃ o cÃ´ng sá»©c cá»§a Ä‘á»™i ngÅ© PyTorch vÃ  cÃ¡c thÆ° viá»‡n Ä‘á»i trÆ°á»›c. Tá»•ng quan, má»™t chÆ°Æ¡ng trÃ¬nh deep learning sáº½ gá»“m cÃ¡c pháº§n nhÆ° sau:

-- Äá»‹nh nghÄ©a flag.

-- Táº£i dá»¯ liá»‡u.

-- Äá»‹nh nghÄ©a model.

-- VÃ²ng láº·p train.

-- VÃ²ng láº·p test.

a. Äá»‹nh nghÄ©a flag:Â Ä‘Ã£ nÃ³i Ä‘áº¿n á»Ÿ pháº§n trÆ°á»›c.

b. Táº£i dá»¯ liá»‡u:

|

1

2

3

4

5

6

7

8

9

10

11

12

13

 |

`train_loader` `=` `torch.utils.data.DataLoader(`

`datasets.MNIST(``'./data'``, train``=``True``, download``=``not` `args.no_download_data,`

`transform``=``transforms.Compose([`

`transforms.ToTensor(),`

`transforms.Normalize((``0.1307``,), (``0.3081``,))`

`])),`

`batch_size``=``args.batch_size, shuffle``=``True``,` `*``*``kwargs)`

`test_loader` `=` `torch.utils.data.DataLoader(`

`datasets.MNIST(``'./data'``, train``=``False``, transform``=``transforms.Compose([`

`transforms.ToTensor(),`

`transforms.Normalize((``0.1307``,), (``0.3081``,))`

`])),`

`batch_size``=``args.batch_size, shuffle``=``True``,` `*``*``kwargs)`

 |

Nhiá»‡m vá»¥ cá»§a pháº§n nÃ y lÃ Â Ä‘á»c dá»¯ liá»‡u vÃ o vÃ  chia chÃºng thÃ nh cÃ¡c batchÂ Ä‘á»ƒ lÃ m input cho model. CÃ¡c batch nÃ y Ä‘Æ°á»£c gá»™p láº¡i trong má»™t loader ("train_loader" hoáº·c "test_loader"). CÃ¡c báº¡n hÃ¬nh dung má»—i loader lÃ  má»™t máº£ng cÃ³ nhiá»u pháº§n tá»­, má»—i pháº§n tá»­ lÃ  má»™t batch. Má»—i batch láº¡i lÃ  má»™t máº£ng cÃ³ nhiá»u pháº§n tá»­, má»—i pháº§n tá»­ lÃ  má»™t táº¥m áº£nh hoáº·c label cá»§a táº¥m áº£nh. Má»¥c Ä‘Ã­ch cá»§a viá»‡c tá»• chá»©c dá»¯ liá»‡u nhÆ° váº­y lÃ  sao cho cÃ¡c báº¡n cÃ³ thá»ƒÂ láº·p qua tá»«ng pháº§n tá»­ cá»§a loader Ä‘á»ƒ Ä‘i qua tá»«ng batch má»™t. HÃ£y nhÃ¬n vÃ o methodÂ [def train(epoch)](https://github.com/khanhptnk/mnist-pytorch/blob/master/main.py#L93)Â taÂ sáº½ tháº¥y Ä‘Æ°á»£c vÃ²ng láº·p nÃ y:

|

1

 |

`for` `batch_idx, (data, target)` `in` `enumerate``(train_loader):`

 |

CÃ¡c biáº¿n "data" vÃ  "target" láº§n lÆ°á»£t chá»©a má»™t batch gá»“m nhiá»u táº¥m áº£nh vÃ  label tÆ°Æ¡ng á»©ng cá»§a chÃºng.

CÃ¡c báº¡n Ä‘á»«ng Ä‘á»ƒ Ã½ Ä‘áº¿n nhá»¯ng chi tiáº¿t phá»©c táº¡p khÃ¡c nhÆ° lÃ  cÃ¡ch khai bÃ¡o loader, cÃ¡c tham sá»‘,... Náº¿u cÃ¡c báº¡n sá»­ dá»¥ng láº¡i code cá»§a ngÆ°á»i khÃ¡c thÃ¬ thÆ°á»ng lÃ  pháº§n táº£i dá»¯ liá»‡u nÃ yÂ Ä‘Æ°á»£c viáº¿t sáºµn. Äá»ƒ thay Ä‘á»•i báº±ng dá»¯ liá»‡u cá»§a cÃ¡c báº¡n, khÃ´ng cáº§n viáº¿t láº¡i code mÃ  chá»‰ cáº§n chá»‰nh láº¡i format cá»§a file dá»¯ liá»‡u vÃ o cho Ä‘Ãºng vá»›i format chuáº©n Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bá»Ÿi ngÆ°á»i viáº¿t code Ä‘Ã³.

c. Äá»‹nh nghÄ©a model:

|

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

 |

`class` `Net(nn.Module):`

`def` `__init__(``self``):`

`super``(Net,` `self``).__init__()`

`self``.conv1` `=` `nn.Conv2d(``1``,` `10``, kernel_size``=``5``)`

`self``.conv2` `=` `nn.Conv2d(``10``,` `20``, kernel_size``=``5``)`

`self``.conv2_drop` `=` `nn.Dropout2d()`

`self``.fc1` `=` `nn.Linear(``320``,` `50``)`

`self``.fc2` `=` `nn.Linear(``50``,` `10``)`

`def` `forward(``self``, x):`

`x` `=` `F.relu(F.max_pool2d(``self``.conv1(x),` `2``))`

`x` `=` `F.relu(F.max_pool2d(``self``.conv2_drop(``self``.conv2(x)),` `2``))`

`x` `=` `x.view(``-``1``,` `320``)`

`x` `=` `F.relu(``self``.fc1(x))`

`x` `=` `F.dropout(x, training``=``self``.training)`

`x` `=` `F.relu(``self``.fc2(x))`

`return` `F.log_softmax(x)`

`model` `=` `Net()`

`if` `args.cuda:`

`model.cuda()`

`optimizer` `=` `optim.SGD(model.parameters(), lr``=``args.lr, momentum``=``args.momentum)`

 |

Náº¿u cÃ¡c báº¡n lÃ m nghiÃªn cá»©u thÃ¬ pháº§n lá»›n cÃ¡c thay Ä‘á»•iÂ sáº½ rÆ¡i vÃ o pháº§n nÃ y. NhÆ° mÃ¬nh Ä‘Ã£ phÃ¢n tÃ­ch á»ŸÂ [bÃ i nÃ y](http://khanhxnguyen.com/machine-learning-101-supervised-learning-perspective/)Â thÃ¬ supervised learning cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t dáº¡ngÂ tá»‘i Æ°u hÃ m sá»‘. Model cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t hÃ m sá»‘Â fÎ¸(x)fÎ¸(x)Â vá»›iÂ xxÂ lÃ  input vÃ Â Î¸Î¸Â lÃ  parameter (tham sá»‘). Äá»‹nh nghÄ©a model chÃ­nh lÃ  Ä‘á»‹nh nghÄ©a hÃ m sá»‘ nÃ y. Vá»›i MNIST, model nháº­n vÃ o má»™t hÃ¬nh áº£nh vÃ , thÃ´ng qua Ä‘á»‹nh nghÄ©a cá»§a mÃ¬nh, tÃ­nh ra Ä‘á»™ cháº¯c cháº¯n (lÃ  má»™t con sá»‘) Ä‘á»‘i vá»›i tá»«ng label tá»« 0 Ä‘áº¿n 9. Muá»‘n dá»± Ä‘oÃ¡n cho táº¥m áº£nh, ta chá»‰ cáº§n láº¥y label cÃ³ Ä‘á»™ cháº¯n cháº¯n cao nháº¥t.

Model á»Ÿ Ä‘Ã¢y Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a báº±ng má»™tÂ [class Net](https://github.com/khanhptnk/mnist-pytorch/blob/master/main.py#L66). Class nÃ y cÃ³ hai method:Â *[__init__()](https://github.com/khanhptnk/mnist-pytorch/blob/master/main.py#L66)*Â vÃ Â [*forward(x)*](https://github.com/khanhptnk/mnist-pytorch/blob/master/main.py#L66).Â *__init__()*Â lÃ  constructor cá»§a class, nÆ¡i Ä‘á»‹nh nghÄ©a cÃ¡c parameter.Â *forward(x)*Â lÃ  nÆ¡i ta Ä‘á»‹nh nghÄ©a cÃ¡c phÃ©p tÃ­nh Ä‘á»ƒ tÃ­nh Ä‘á»™ cháº¯c cháº¯n tá»« input.

Äá»ƒ dá»… hiá»ƒu hÆ¡n, táº¡m quÃªn Ä‘i MNIST, mÃ¬nh giáº£ sá»­ model lÃ  má»™t Ä‘a thá»©c báº­c 2Â fÎ¸(x)=ax2+bx+cfÎ¸(x)=ax2+bx+cÂ vá»›i tham sá»‘Â Î¸=(a,b,c)Î¸=(a,b,c). ChÃºng ta sáº½ Ä‘á»‹nh nghÄ©a model nhÆ° sau:

|

1

2

3

4

5

6

 |

`class` `Net(nn.Module):`

`def` `__init__(``self``):`

`khai bÃ¡o parameter lÃ  a, b, c.`

`def` `forward(``self``, x):`

`return` `a``*``x^``2` `+` `b``*``x` `+` `c`

 |

Sau khi Ä‘á»‹nh nghÄ©a class cá»§a model, taÂ táº¡o ra má»™t object model:

|

1

 |

`model` `=` `Net()`

 |

Dá»±a vÃ o Ä‘á»™ cháº¯c cháº¯n cá»§a model vÃ  Ä‘Ã¡p Ã¡n Ä‘Ãºng, ta sáº½ tÃ­nh hÃ m máº¥t mÃ¡t. Sau Ä‘Ã³ taÂ cáº§n má»™t optimizer Ä‘á»ƒ tÃ¬m ra parameter tá»‘i Æ°u cá»§a modelÂ sao cho hÃ m máº¥t mÃ¡t nÃ y Ä‘áº¡t cá»±c tiá»ƒu.

|

1

 |

`optimizer` `=` `optim.SGD(model.parameters(), lr``=``args.lr, momentum``=``args.momentum)`

 |

Khi táº¡o object optimizer, báº¡n pháº£i truyá»n vÃ o táº¥t cáº£ parameter cá»§a model muá»‘n Ä‘Æ°á»£c tá»‘i Æ°u (gá»iÂ *model.parameters()*). CÃ¡c thÃ´ng sá»‘ khÃ¡c nhÆ° learning rate hoáº·c lÃ  momentum lÃ  tÃ¹y vÃ o loáº¡i optimizer Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng. á» Ä‘Ã¢y, ta Ä‘ang dÃ¹ng momentum SGD.

Äiá»u kÃ¬ diá»‡u cá»§a cÃ¡c optimizer nÃ y lÃ  chÃºng sáº½Â tá»± Ä‘á»™ng dÃ¹ng backpropagation Ä‘á»ƒ tÃ­nh Ä‘áº¡o hÃ m cá»§a hÃ m máº¥t mÃ¡t theo tá»«ng tham sá»‘ vÃ  thay Ä‘á»•i tham sá»‘ theo Ä‘áº¡o hÃ m. ChÃºng ta khÃ´ng cáº§n pháº£i ngá»“i tá»± viáº¿t cÃ´ng thá»©c Ä‘áº¡o hÃ m, vá»«a dÃ i vá»«a dá»… sai.

d. VÃ²ng láº·p train:

|

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

 |

`def` `train(epoch):`

`model.train()`

`for` `batch_idx, (data, target)` `in` `enumerate``(train_loader):`

`if` `args.cuda:`

`data, target` `=` `data.cuda(), target.cuda()`

`data, target` `=` `Variable(data), Variable(target)`

`optimizer.zero_grad()`

`output` `=` `model(data)`

`loss` `=` `F.nll_loss(output, target)`

`loss.backward()`

`optimizer.step()`

`if` `batch_idx` `%` `args.log_interval` `=``=` `0``:`

`print``(``'Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'``.``format``(`

`epoch, batch_idx` `*` `len``(data),` `len``(train_loader.dataset),`

`100.` `*` `batch_idx` `/` `len``(train_loader), loss.data[``0``]))`

 |

Bá»‘ cá»¥c chÃ­nh cá»§a vÃ²ng láº·p train nhÆ° sau:

|

1

2

3

4

5

6

7

8

 |

`for` `má»—i batch (data, target) (data lÃ  x, target lÃ  y):`

`1.` `ÄÆ°a data vÃ o lÃ m` `input` `cho model vÃ  nháº­n vá» output:` `-``-` `output` `=` `model(data)` `-``-`

`2.` `TÃ­nh hÃ m máº¥t mÃ¡t dá»±a vÃ o output vÃ  label Ä‘Ãºng:` `-``-` `loss` `=` `F.nll_loss(output, target)` `-``-`

`3.` `Chá»‰nh láº¡i tham sá»‘ cá»§a model báº±ng viá»‡c gá»i optimizer:`

`a. Bá» háº¿t Ä‘áº¡o hÃ m cÅ© Ä‘i:` `-``-` `optimizer.zero_grad()` `-``-`

`b. DÃ¹ng backpropagation tÃ­nh Ä‘áº¡o hÃ m theo tá»«ng tham sá»‘:` `-``-` `loss.backward()` `-``-`

`c. Thay Ä‘á»•i tham sá»‘ dá»±a vÃ o Ä‘áº¡o hÃ m:` `-``-` `optimizer.step()` `-``-`

`4.` `ThÃ´ng bÃ¡o loss trÃªn batch vá»«a xá»­ lÃ½.`

 |

CÃ¡c báº¡n tháº¥y lÃ  á»Ÿ Ä‘Ã¢y model khÃ´ng gá»i hÃ mÂ *model.forward(x)*, mÃ  chá»‰ Ä‘Æ¡n giáº£n lÃ Â *model(data)*. Tuy nhiÃªn, Ä‘Ã¢y chá»‰ lÃ  má»™t máº¹o láº­p trÃ¬nh Ä‘á»ƒ rÃºt gá»n code mÃ  thÃ´i.Â Äá»‹nh nghÄ©a cá»§a model pháº£i náº±m trong hÃ mÂ *forward(x)Â *vÃ  cÃ¡c báº¡n khÃ´ng Ä‘Æ°á»£c Ä‘áº·t tÃªn hÃ m nÃ y khÃ¡c Ä‘i.

e. VÃ²ng láº·p test:

|

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

 |

`def` `test(epoch):`

`model.``eval``()`

`test_loss` `=` `0`

`correct` `=` `0`

`for` `data, target` `in` `test_loader:`

`if` `args.cuda:`

`data, target` `=` `data.cuda(), target.cuda()`

`data, target` `=` `Variable(data, volatile``=``True``), Variable(target)`

`output` `=` `model(data)`

`test_loss` `+``=` `F.nll_loss(output, target).data[``0``]`

`pred` `=` `output.data.``max``(``1``)[``1``]` `# get the index of the max log-probability`

`correct` `+``=` `pred.eq(target.data).cpu().``sum``()`

`test_loss` `=` `test_loss`

`test_loss` `/``=` `len``(test_loader)` `# loss function already averages over batch size`

`print``(``'\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'``.``format``(`

`test_loss, correct,` `len``(test_loader.dataset),`

`100.` `*` `correct` `/` `len``(test_loader.dataset)))`

 |

VÃ²ng láº·p test gáº§n nhÆ° tÆ°Æ¡ng tá»± nhÆ° vÃ²ng láº·p train tuy nhiÃªn cÃ³ má»™t sá»‘ sá»± khÃ¡c biá»‡t sau:\
-- Sau khi train, báº¡n Ä‘Ã£ tÃ¬m Ä‘Æ°á»£c tham sá»‘ cá»§a model rá»“i. BÃ¢y giá» báº¡nÂ khÃ´ng cáº§n dÃ¹ng optimizer Ä‘á»ƒ thay Ä‘á»•i tham sá»‘Â ná»¯a mÃ  chá»‰ viá»‡c sá»­ dá»¥ng model nhÆ° má»™t hÃ m sá»‘ thÃ´ng thÆ°á»ng, tÃ­nh output ra tá»« cÃ¡c input.\
-- NgoÃ i viá»‡c tÃ­nh hÃ m máº¥t mÃ¡t, báº¡n cÃ²n pháº£iÂ tÃ­nh ra metric báº¡n tháº­t sá»± quan tÃ¢mÂ (á»Ÿ Ä‘Ã¢y lÃ  accuracy, pháº§n trÄƒm bao nhiÃªu vÃ­ dá»¥ Ä‘Æ°á»£c Ä‘oÃ¡n Ä‘Ãºng):

|

1

2

 |

`pred` `=` `output.data.``max``(``1``)[``1``]` `# get the index of the max log-probability`

`correct` `+``=` `pred.eq(target.data).cpu().``sum``()`

 |

Cuá»‘i cÃ¹ng, vÃ¬ vÃ²ng láº·p train vÃ  test chá»‰ lÃ  vÃ²ng láº·p cho má»™t epoch mÃ  thÃ´i, chÃºng ta cÃ³ má»™t vÃ²ng láº·p á»Ÿ cuá»‘i chÆ°Æ¡ng trÃ¬nh Ä‘á»ƒ gá»i vÃ²ng láº·p train vÃ  test cho má»—i epoch.

|

1

2

3

 |

`for` `epoch` `in` `range``(``1``, args.epochs` `+` `1``):`

`train(epoch)`

`test(epoch)`

 |

BÃ i táº­p:Â *hÃ£y lÃ m cho model trá»Ÿ nÃªn "deep" báº±ng cÃ¡ch thÃªm nhiá»u layer vÃ o trong Ä‘á»‹nh nghÄ©a cá»§a nÃ³.Â *Gá»£i Ã½:*Â thay Ä‘á»•i hÃ mÂ *[forward(x)](https://github.com/khanhptnk/mnist-pytorch/blob/master/main.py#L75)*.*

BÃ i táº­p:Â *tÃ¬m hiá»ƒu Ã½ nghÄ©a cá»§a cÃ¡c hÃ mÂ *nn.Conv2d()*,Â *nn.Dropout2d()*,Â *nn.Linear()*Â trong hÃ mÂ *[forward(x)](https://github.com/khanhptnk/mnist-pytorch/blob/master/main.py#L75)*. Gá»£i Ã½: search tÃªn chÃºng á»ŸÂ [PyTorch API reference](http://pytorch.org/api/0.1.3/en/#introduction).Â *

BÃ i viáº¿t Ä‘áº¿n Ä‘Ã¢y lÃ  háº¿t. ÄÃ¢y lÃ  bÃ i dÃ i nháº¥t mÃ¬nh tá»«ng viáº¿t, náº¿u cÃ³ sai sÃ³t gÃ¬ má»i ngÆ°á»i comment Ä‘á»ƒ mÃ¬nh sá»­a láº¡i. LÃ m háº¿t bÃ i nÃ y cháº¯c cÅ©ng bá»Ÿ hÆ¡i tai nhÆ°ng hy vá»ng cÃ¡c báº¡n sáº½ cÃ³ nhiá»u niá»m vui. Happy codingÂ ![ğŸ™‚](https://s.w.org/images/core/emoji/11/svg/1f642.svg)