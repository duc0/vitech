{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd: Tính đạo hàm tự động\n",
    "===================================\n",
    "\n",
    "Về mặt toán học thì mạng neural có thể được xem như một hàm toán học phức tạp, và việc tính đạo hàm của nó là rất quan trọng khi huấn luyện mạng neural.\n",
    "\n",
    "Vì vậy ``autograd`` hay chức năng tính đạo hàm tự động là một chức năng vô cùng quan trọng của Pytorch. Nói ngắn gọn, thì ``autograd`` giúp ta tính được giá trị đạo hàm tại các điểm cụ thể với hàm số được tạo ra từ mọi phép toán trên Tensor mà không cần phải biết công thức chính xác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhắc lại về đạo hàm\n",
    "--------\n",
    "\n",
    "Đạo hàm của một hàm số mô tả sự biến thiên của một hàm số tại một điểm, ví dụ \n",
    "cho hàm số $y =3x^2$ thì đạo hàm $y'=6x$.\n",
    "Tại điểm cụ thể $x = 5$, ta có $y = 75$ và $y' = 30$ (giải thích nôm na là, khi x thay đổi 1 đơn vị, thì y thay đổi 30 đơn vị, trong giới hạn nhỏ quanh điểm $x=5$). Ví dụ tính toán đơn giản này có thể được mô tả trong pytorch như sau:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([75.], grad_fn=<ThMulBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo điểm x = 5\n",
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "y = 3 * x * x\n",
    "# Tính y tại điểm x = 5\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30.])\n"
     ]
    }
   ],
   "source": [
    "# Tính đạo hàm\n",
    "y.backward()\n",
    "# In ra giá trị đạo hàm y' = dy/dx tại điểm x = 5\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy là pytorch giúp ta tính được giá trị của đạo hàm $y'$ tại một điểm x mà không cần dùng công thức đạo hàm $x^n = nx^{n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta có thể mở rộng ra việc tính đạo hàm với hàm nhiều biến với ví dụ sau, cho hàm số $z = 5x^3 + 2y^2$, ta có đạo hàm riêng $\\frac{dz}{dx}=15x^2$, và $\\frac{dz}{dy}=4y$. Tại điểm $(x = 7, y = 8)$ ta có $z = 1843$ và $\\frac{dz}{dx}=735$, $\\frac{dz}{dy}=32$\n",
    "\n",
    "Ví dụ này được mô tả trong pytorch như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1843.], grad_fn=<ThAddBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo điểm x = 5\n",
    "x = torch.tensor([7.0], requires_grad=True)\n",
    "y = torch.tensor([8.0], requires_grad=True)\n",
    "z = 5 * x * x * x + 2 * y * y\n",
    "# Tính z tại điểm (x = 7, y = 8)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([735.])\n"
     ]
    }
   ],
   "source": [
    "# Tính đạo hàm\n",
    "z.backward()\n",
    "# In ra giá trị đạo hàm dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([32.])\n"
     ]
    }
   ],
   "source": [
    "# In ra giá trị đạo hàm dz/dy\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý trong các ví dụ trên, khi khai báo các tensor như x, y và sau này cần tính đạo hàm, ta cần thêm thuộc tính `requires_grad=True` để pytorch ghi nhớ các phép toán trên các tensor này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đạo hàm của Tensor\n",
    "--------\n",
    "Ở các ví dụ trên đây, ta đã khai báo x, y như các đại lượng vô hướng (Tensor với kích thước 1), vậy trong trường hợp tổng quát thì sao? Ta hãy quay lại ví dụ đầu tiên nhưng với x là 1 ma trận 2x2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 48.],\n",
      "        [75., 12.]], grad_fn=<ThMulBackward>)\n",
      "tensor([[18., 24.],\n",
      "        [30., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[3.0, 4.0], [5.0, 2.0]], requires_grad=True)\n",
    "y = 3 * x * x\n",
    "print(y)\n",
    "y = y.sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý, trong ví dụ trên đây, $x*x$ chỉ phép lấy tích hai ma trận theo từng phân tử, chứ không phải là phép nhân ma trận. Ta thấy trong trường hợp này, kết quả đạo hàm $y'$ chỉ là mở rộng của trường hợp đại lượng vô hướng; mỗi phần tử của $y'$ là giá trị đạo hàm tại điểm tương ứng trên ma trận $x$ ban đầu. Ta hãy thử thay $x*x$ bằng phép nhân ma trận (thể hiện qua hàm `torch.mm`) và xem giá trị đạo hàm thay đổi như thế nào. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[87., 60.],\n",
      "        [75., 72.]], grad_fn=<MulBackward>)\n",
      "tensor([[45., 45.],\n",
      "        [39., 39.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[3.0, 4.0], [5.0, 2.0]], requires_grad=True)\n",
    "y = 3 * torch.mm(x, x)\n",
    "print(y)\n",
    "y = y.sum()\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong ví dụ trên, giả sử\n",
    "$\n",
    "x = \\left(\\begin{array} \n",
    "\\\\a & b\\\\\n",
    "c & d\n",
    "\\end{array}\\right)\n",
    "$, thì ta sẽ có $y = 3 * (a^2 + d^2 + ab + ac + bc + bd + cd)$, và đạo hàm riêng $\\frac{dy}{da}=6a + 3b + 3c$, với $\n",
    "x = \\left(\\begin{array} \n",
    "\\\\3 & 4\\\\\n",
    "5 & 2\n",
    "\\end{array}\\right)\n",
    "$ thì  $\\frac{dy}{da}=45$\n",
    "\n",
    "Ghi chú: ``autograd`` cũng hoạt động được khi y không phải là đại lượng vô hướng; tuy nhiên điều này nằm ngoài phạm vi của bài này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd với hàm hợp\n",
    "--------\n",
    "Giá trị của autograd thể hiện rõ thêm với một hàm số hợp được định nghĩa qua nhiều bước (đặc biệt hay áp dụng trong mạng Neural khi kết quả là sự kếp hợp của nhiều lớp Neural). Ví dụ $y=2x^3$ và $z=5y^2$, nếu tính đạo hàm thủ công ta có thể dùng công thức $\\frac{dz}{dx}=\\frac{dz}{dy}\\frac{dy}{dx}=10y6x^2=120x^5$. Tại $x=2$ thì $\\frac{dz}{dx}=3840$. Ví dụ này được mô tả qua pytorch như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16.], grad_fn=<ThMulBackward>)\n",
      "tensor([1280.], grad_fn=<ThMulBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo điểm x = 2\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = 2 * x * x * x\n",
    "z = 5 * y * y\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3840.])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd hoạt động như thế nào\n",
    "--------\n",
    "Ta có thể giải thích cơ chế hoạt động của autograd từ ví dụ đơn giản trên đây. Autograd sẽ xây dựng hàm số đích dưới dạng một đồ thị tính toán (computation graph) qua từng bước, và cũng áp dụng công thức $\\frac{dz}{dx}=\\frac{dz}{dy}\\frac{dy}{dx}$ để tính ngược đạo hàm. Lưu ý là autograd giúp ta tính giá trị cụ thể của đạo hàm tại một điểm xác định nên công thức này sẽ áp dụng trên giái trị cụ thể, chứ không phải trên công thức tổng quát (symbolic differentiation). Trong ví dụ trên, đầu tiên autograd sẽ tính $\\frac{dz}{dy} = 160$, sau đó tính được $\\frac{dy}{dx} = 24$, và cuối cùng $\\frac{dz}{dx}=3840$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
